import torch
from torch import nn
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from torch_geometric.nn import GCNConv, GAT
import torch.nn.functional as F
from scipy.io import loadmat
import pickle as pkl
import numpy as np
from tqdm import tqdm
from sklearn.metrics import f1_score
import random

from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import copy
import os
import yaml
import sys


from utils import *
from models import GCN, Simpler_GCN, Simpler_GCN_Conv, GCN_Att, Simpler_GCN2, GCN_Att_Drop_Multihead, GCN_Att_Not_res, GAT_Edge_feat, PNA_model

"""
The script is used to train a model on the Yelp or Amazon 
dataset for anomaly detection in a supervised manner.

It first uses Triplet loss to train the model to generate good 
node representations. Then, it trains a classifier on top of
the learned embeddings using Cross entropy loss.
"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print('Device:', device)

# Get the name of the yaml file
name_yaml = sys.argv[1]
print(f'Running {name_yaml}')

# Open a yaml file with the parameters
with open(f'./Setups/SupervisedContrastive/{name_yaml}.yaml') as file:
    params = yaml.load(file, Loader=yaml.FullLoader)

# Load the data and get the path of the run (It also creates the folder structure to save the experiment results)
graph, run_path, train_mask, val_mask, test_mask, train_mask_contrastive = preprocess_data(params, name_yaml, "SupervisedContrastive")

# Load the specified model
if params["model_name"] == 'Simpler_GCN':
    model = Simpler_GCN(**params['model'])
elif params["model_name"] == 'Simpler_GCN_Conv':
    model = Simpler_GCN_Conv(**params['model'])
elif params["model_name"] == 'Simpler_GCN2':
    model = Simpler_GCN2(**params['model'])
elif params["model_name"] == 'GCN_Att':
    model = GCN_Att(**params['model'])
elif params["model_name"] == 'GCN_Att_Drop_Multihead':
    model = GCN_Att_Drop_Multihead(**params['model'])
elif params["model_name"] == 'GCN_Att_Not_res':
    model = GCN_Att_Not_res(**params['model'])
elif params["model_name"] == 'GAT_Edge_feat':
    model = GAT_Edge_feat(**params['model'])
elif params["model_name"] == 'PNA_model':
    model = PNA_model(**params['model'])
else:
    raise ValueError(f'{params["model_name"]} is not a valid model name')

# Move the model into cuda if available
model = model.to(device)
graph = graph.to(device)

# Define the optimizer
optimizer_gcn = torch.optim.AdamW(model.parameters(), lr=params["lr"], weight_decay=params["weight_decay"])

# Train the model if the flag is set
if params["train_contrastive"]:
    model = train_node_embedder_supervised(model, graph, optimizer_gcn, 
                                        config=params,
                                        name_model=f'{run_path}/Weights/contr_sup_{name_yaml}.pth')
else:
    model.load_state_dict(torch.load(f'{run_path}/Weights/contr_sup_{name_yaml}.pth', map_location=device))

# Get the embeddings of the nodes
model.eval()
with torch.no_grad():
    out = model.contrastive(graph)
    out = out.cpu().numpy()
    labels = graph.y.cpu().numpy()

# Save the embeddings
with open(f'{run_path}/Pickles/embeds_contr_sup_{name_yaml}.pkl', 'wb') as file:
    pkl.dump(out, file)
# Save the masks used for training, validation and testing
with open(f"{run_path}/Pickles/train_test_val_masks_{name_yaml}.pkl", "wb") as file:
    pkl.dump([train_mask, val_mask, test_mask, train_mask_contrastive], file)


# If the embeddings are more than 2D, we apply t-SNE to reduce them to 2D
if out.shape[1] > 2:
    # Applying t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(out[3305:])  # Assuming the first 3304 are unlabeled and hence excluded


    # Separating the reduced features by their labels for plotting
    X_tsne_benign = X_tsne[labels[3305:] == 0]
    X_tsne_fraudulent = X_tsne[labels[3305:] == 1]

    # Plotting all the nodes embeddings
    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model')
    plt.xlabel('t-SNE feature 1')
    plt.ylabel('t-SNE feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_drop_{name_yaml}_all.png')
    plt.close()

    # Plotting only the test nodes
    X_tsne_test = X_tsne[graph.test_mask.cpu().numpy()[3305:], :]
    labels_test = labels[3305:][graph.test_mask.cpu().numpy()[3305:]]
    X_tsne_benign = X_tsne_test[labels_test == 0]
    X_tsne_fraudulent = X_tsne_test[labels_test == 1]

    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model (Test nodes only)')
    plt.xlabel('t-SNE feature 1')
    plt.ylabel('t-SNE feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_{name_yaml}_test.png')
    plt.close()

    # Plotting only the train nodes
    X_tsne_train = X_tsne[graph.train_mask.cpu().numpy()[3305:], :]
    labels_train = labels[3305:][graph.train_mask.cpu().numpy()[3305:]]
    X_tsne_benign = X_tsne_train[labels_train == 0]
    X_tsne_fraudulent = X_tsne_train[labels_train == 1]

    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model (Train nodes only)')
    plt.xlabel('t-SNE feature 1')
    plt.ylabel('t-SNE feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_{name_yaml}_train.png')
    plt.close()
else:
    # As the embeddings are already 2D, we can directly plot them

    X_tsne = out[3305:] # Assuming the first 3304 are unlabeled and hence excluded

    # Separating the reduced features by their labels for plotting
    X_tsne_benign = X_tsne[labels[3305:] == 0]
    X_tsne_fraudulent = X_tsne[labels[3305:] == 1]

    # Plotting all the nodes embeddings
    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_{name_yaml}_all.png')
    plt.close()

    X_tsne = out

    # Plotting only the test nodes
    X_tsne_test = X_tsne[graph.test_mask.cpu().numpy(), :]
    labels_test = labels[graph.test_mask.cpu().numpy()]
    X_tsne_benign = X_tsne_test[labels_test == 0]
    X_tsne_fraudulent = X_tsne_test[labels_test == 1]

    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model (Test nodes only)')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_{name_yaml}_test.png')
    plt.close()

    # Plotting only the train nodes
    X_tsne_train = X_tsne[graph.train_mask.cpu().numpy(), :]
    labels_train = labels[graph.train_mask.cpu().numpy()]
    X_tsne_benign = X_tsne_train[labels_train == 0]
    X_tsne_fraudulent = X_tsne_train[labels_train == 1]

    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne_benign[:, 0], X_tsne_benign[:, 1], label='Benign (Class 0)', alpha=0.5)
    plt.scatter(X_tsne_fraudulent[:, 0], X_tsne_fraudulent[:, 1], label='Fraudulent (Class 1)', alpha=0.5)
    plt.title('t-SNE visualization of the node embeddings generated by the contrastive model (Train nodes only)')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.savefig(f'{run_path}/Plots/embeds_contr_sup_{name_yaml}_train.png')
    plt.close()


# If the flag is set, train the classification head
if params["train_head"]:
    # Frozing all the model parameters that are not on the classification head
    for name, param in model.named_parameters():
        if 'classifier' not in name:
            param.requires_grad = False

    # Get the parameters that require grad    
    parameters = filter(lambda p: p.requires_grad, model.parameters())

    # Define the optimizer
    optimizer_gcn = torch.optim.AdamW(parameters, lr=params["lr"], weight_decay=params["weight_decay"])

    # Compute the class weights for the loss function
    # In order to balance the classes weight on the loss function
    train_samples = graph.y[graph.train_mask]
    weight_for_class_0 = len(train_samples) / (len(train_samples[train_samples == 0]) * 2)
    weight_for_class_1 = len(train_samples) / (len(train_samples[train_samples == 1]) * 2)
    criterion = nn.CrossEntropyLoss(weight=torch.tensor([weight_for_class_0, weight_for_class_1]).to(device))

    # Train the model classifier
    model = train_node_classifier_minibatches(model=model, graph=graph, config=params, 
                                            criterion=criterion, optimizer=optimizer_gcn, only_head=True, 
                                            name_model=f'{run_path}/Weights/head_contr_sup_{name_yaml}.pth')
    # Load the best model
    model.load_state_dict(torch.load(f'{run_path}/Weights/head_contr_sup_{name_yaml}.pth'))

    # Evaluate the model and get the predictions
    test_acc, f1, predictions = eval_node_classifier(model, graph, graph.test_mask)
    print(f'Test Acc: {test_acc:.3f}, Test F1: {f1:.3f}')

    # Compute the confusion matrix
    conf_matrix = confusion_matrix(graph.y[graph.test_mask].cpu().numpy(),
                                predictions[graph.test_mask].cpu().numpy())
    sns.heatmap(conf_matrix, annot=True, fmt='d')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig(f'{run_path}/Plots/cm_contr_sup_{name_yaml}.png')
    plt.close()

    # Compute the classification report, it includes precision, recall, f1-score for each class
    report = classification_report(graph.y[graph.test_mask].cpu().numpy(), predictions[graph.test_mask].cpu().numpy(), output_dict=True)

    # Computing the area under the curve and the average precision scores
    report["ROC_AUC"] = compute_ROC_AUC(model, graph, graph.test_mask)
    report["AP"] = compute_Average_Precision(model, graph, graph.test_mask)

    # Computing the ROC curve
    fpr, tpr = compute_ROC_curve(model, graph, graph.test_mask)
    # Plotting the ROC curve
    plt.figure()
    lw = 2
    plt.plot(
        fpr,
        tpr,
        color="darkorange",
        lw=lw,
        label="ROC curve",
    )
    plt.plot([0, 1], [0, 1], color="navy", lw=lw, linestyle="--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.savefig(f'{run_path}/Plots/roc_curve_{name_yaml}.png')
    plt.close()

    # Computing the Precision-Recall curve
    precision, recall = compute_PR_curve(model, graph, graph.test_mask)

    # Plotting the Precision-Recall curve
    plt.figure()
    lw = 2
    plt.plot(
        recall,
        precision,
        color="darkorange",
        lw=lw,
        label="Precision-Recall curve",
    )
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall curve")
    plt.savefig(f'{run_path}/Plots/PR_curve_{name_yaml}.png')
    plt.close()


    # Save the classification metrics
    with open(f'{run_path}/Report/contr_sup_{name_yaml}.txt', 'w') as file:
        file.write(str(report))

if "onlyEvaluate" in params and params["onlyEvaluate"]:
    # Load the best model
    model.load_state_dict(torch.load(f'{run_path}/Weights/head_contr_sup_{name_yaml}.pth', map_location=device))

    # Evaluate the model and get the predictions
    test_acc, f1, predictions = eval_node_classifier(model, graph, graph.test_mask)
    print(f'Test Acc: {test_acc:.3f}, Test F1: {f1:.3f}')

    # Compute the confusion matrix
    conf_matrix = confusion_matrix(graph.y[graph.test_mask].cpu().numpy(),
                                predictions[graph.test_mask].cpu().numpy())
    sns.heatmap(conf_matrix, annot=True, fmt='d')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig(f'{run_path}/Plots/cm_contr_sup_{name_yaml}.png')
    plt.close()

    # Compute the classification report, it includes precision, recall, f1-score for each class
    report = classification_report(graph.y[graph.test_mask].cpu().numpy(), predictions[graph.test_mask].cpu().numpy(), output_dict=True)

    # Computing the area under the curve and the average precision scores
    report["ROC_AUC"] = compute_ROC_AUC(model, graph, graph.test_mask)
    report["AP"] = compute_Average_Precision(model, graph, graph.test_mask)

    # Computing the ROC curve
    fpr, tpr = compute_ROC_curve(model, graph, graph.test_mask)
    # Plotting the ROC curve
    plt.figure()
    lw = 2
    plt.plot(
        fpr,
        tpr,
        color="darkorange",
        lw=lw,
        label="ROC curve",
    )
    plt.plot([0, 1], [0, 1], color="navy", lw=lw, linestyle="--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.savefig(f'{run_path}/Plots/roc_curve_{name_yaml}.png')
    plt.close()

    # Computing the Precision-Recall curve
    precision, recall = compute_PR_curve(model, graph, graph.test_mask)

    # Plotting the Precision-Recall curve
    plt.figure()
    lw = 2
    plt.plot(
        recall,
        precision,
        color="darkorange",
        lw=lw,
        label="Precision-Recall curve",
    )
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall curve")
    plt.savefig(f'{run_path}/Plots/PR_curve_{name_yaml}.png')
    plt.close()


    # Save the classification metrics
    with open(f'{run_path}/Report/contr_sup_{name_yaml}.txt', 'w') as file:
        file.write(str(report))